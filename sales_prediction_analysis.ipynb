{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b09ac60",
   "metadata": {},
   "source": [
    "# Sales Prediction Analytics Project\n",
    "\n",
    "This notebook provides a full walkthrough of a synthetic retail sales dataset, including exploratory data analysis (EDA), data visualization, and predictive modeling. The project is designed to demonstrate business analytics and data science skills for roles such as **business analyst**, **program manager**, and **data analyst**.\n",
    "\n",
    "The synthetic dataset contains 1,000 records of retail transactions with features like product categories, quantities, prices, discounts, regions, customer demographics, and revenue/profit. We engineer a binary target variable (`high_profit`) indicating whether a transaction's profit is above the median.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style='whitegrid', palette='muted')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('synthetic_sales_data.csv')\n",
    "\n",
    "# Display first few rows\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d777e4e",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We begin by exploring the dataset's basic structure and summary statistics. We inspect distributions of numerical variables like **revenue** and **profit**, and evaluate categorical distributions such as **product_category** and **region**. Visualizations help us identify trends and potential relationships in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abeabf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(data.describe(include='all'))\n",
    "\n",
    "# Plot distribution of revenue and profit\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].hist(data['revenue'], bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Revenue Distribution')\n",
    "axes[0].set_xlabel('Revenue')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(data['profit'], bins=30, color='salmon', edgecolor='black')\n",
    "axes[1].set_title('Profit Distribution')\n",
    "axes[1].set_xlabel('Profit')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot for product categories\n",
    "plt.figure(figsize=(8, 4))\n",
    "data['product_category'].value_counts().plot(kind='bar', color='purple')\n",
    "plt.title('Transaction Counts by Product Category')\n",
    "plt.xlabel('Product Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Bar plot for regions\n",
    "plt.figure(figsize=(6, 4))\n",
    "data['region'].value_counts().plot(kind='bar', color='green')\n",
    "plt.title('Transaction Counts by Region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598cee4",
   "metadata": {},
   "source": [
    "## Feature Engineering & Preprocessing\n",
    "\n",
    "To build a predictive model, we'll prepare our dataset by:\n",
    "\n",
    "1. Selecting relevant features and the target variable (`high_profit`).\n",
    "2. Splitting the data into training and testing sets.\n",
    "3. Applying one-hot encoding to categorical variables.\n",
    "4. Building pipelines for machine learning models.\n",
    "\n",
    "We will evaluate two models:\n",
    "\n",
    "- **Logistic Regression**: A baseline linear classifier.\n",
    "- **Random Forest Classifier**: An ensemble model that can capture non-linear relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c00839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "features = ['region', 'product_category', 'product_subcategory', 'quantity', 'unit_price', 'discount',\n",
    "            'shipping_cost', 'customer_age_group', 'payment_method', 'returned']\n",
    "target = 'high_profit'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['region', 'product_category', 'product_subcategory', 'customer_age_group', 'payment_method']\n",
    "numerical_cols = ['quantity', 'unit_price', 'discount', 'shipping_cost', 'returned']\n",
    "\n",
    "# Preprocess: one-hot encode categorical variables and pass through numerical variables unchanged\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Define models\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create pipelines\n",
    "log_reg_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', log_reg)])\n",
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', rf_clf)])\n",
    "\n",
    "# Train models\n",
    "log_reg_pipeline.fit(X_train, y_train)\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model, X_t, y_t, name):\n",
    "    y_pred = model.predict(X_t)\n",
    "    print(f\"\n",
    "=== {name} ===\")\n",
    "    print(classification_report(y_t, y_pred))\n",
    "    print(\"Confusion Matrix:\n",
    "\", confusion_matrix(y_t, y_pred))\n",
    "\n",
    "# Evaluation on test set\n",
    "evaluate_model(log_reg_pipeline, X_test, y_test, 'Logistic Regression')\n",
    "evaluate_model(rf_pipeline, X_test, y_test, 'Random Forest Classifier')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf679e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we created a **synthetic retail sales dataset** and performed exploratory data analysis to uncover distribution patterns across revenue, profit, product categories, and regions. We then engineered a binary target variable (`high_profit`) and trained two classification models—**Logistic Regression** and **Random Forest**—to predict high-profit transactions.\n",
    "\n",
    "The Random Forest model typically shows higher accuracy due to its ability to capture complex relationships between features. You can further experiment with hyperparameter tuning, cross-validation, and different modeling techniques (e.g., gradient boosting) to improve performance.\n",
    "\n",
    "This project demonstrates essential steps in a data analytics workflow: data generation, EDA, visualization, feature engineering, model building, and evaluation. Feel free to expand upon this foundation by exploring additional insights (e.g., segmenting by customer age group, analyzing return rates) or deploying the model in a production environment.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
